{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06-measure-novelty-from-aggregated-topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "mpl.rcParams['figure.figsize'] = [7,8]\n",
    "mpl.rcParams['figure.dpi'] = 80\n",
    "mpl.rcParams['savefig.dpi'] = 200\n",
    "\n",
    "mpl.rcParams['font.size'] = 17\n",
    "mpl.rcParams['legend.fontsize'] = 'large'\n",
    "mpl.rcParams['figure.titlesize'] = 'medium'\n",
    "mpl.rcParams['lines.linewidth'] = 2.5\n",
    "mpl.rcParams['lines.markersize'] = 10\n",
    "\n",
    "sns.set_context('poster')\n",
    "\n",
    "from pprint import pprint\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import imp\n",
    "\n",
    "from scipy.stats import entropy\n",
    "entropy_fix = np.log2(np.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_week(x):\n",
    "    d1 = parse(x)\n",
    "    d0 = datetime(d1.year,1,1,0,0)\n",
    "    d_gap = d1-d0\n",
    "    week = d_gap.days/7 + 1\n",
    "    return int(week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988 newspieces in 1947\n",
      "732 newspieces in 1948\n",
      "1035 newspieces in 1949\n",
      "2018 newspieces in 1950\n",
      "1915 newspieces in 1951\n",
      "2466 newspieces in 1952\n",
      "2532 newspieces in 1953\n",
      "2555 newspieces in 1954\n",
      "2369 newspieces in 1955\n",
      "1160 newspieces in 1956\n",
      "1650 newspieces in 1957\n",
      "1250 newspieces in 1958\n",
      "1706 newspieces in 1959\n",
      "1385 newspieces in 1960\n",
      "2543 newspieces in 1961\n",
      "1828 newspieces in 1962\n",
      "2575 newspieces in 1963\n",
      "1885 newspieces in 1964\n",
      "3231 newspieces in 1965\n",
      "2031 newspieces in 1966\n",
      "3125 newspieces in 1967\n",
      "1861 newspieces in 1968\n",
      "3054 newspieces in 1969\n",
      "1906 newspieces in 1970\n",
      "1785 newspieces in 1971\n",
      "2057 newspieces in 1972\n",
      "1829 newspieces in 1973\n",
      "1773 newspieces in 1974\n",
      "1885 newspieces in 1975\n",
      "2007 newspieces in 1976\n",
      "3727 newspieces in 1977\n",
      "2214 newspieces in 1978\n",
      "81 newspieces in 1979\n",
      "1738 newspieces in 1980\n",
      "2709 newspieces in 1981\n",
      "2441 newspieces in 1982\n",
      "3375 newspieces in 1983\n",
      "2798 newspieces in 1984\n",
      "2794 newspieces in 1985\n",
      "3056 newspieces in 1986\n",
      "2376 newspieces in 1987\n",
      "3124 newspieces in 1988\n",
      "2175 newspieces in 1989\n",
      "2865 newspieces in 1990\n",
      "1273 newspieces in 1991\n",
      "3290 newspieces in 1992\n",
      "1279 newspieces in 1993\n",
      "3182 newspieces in 1994\n",
      "1567 newspieces in 1995\n",
      "3944 newspieces in 1996\n",
      "6511 newspieces in 1997\n",
      "3989 newspieces in 1998\n",
      "2473 newspieces in 1999\n",
      "5105 newspieces in 2000\n",
      "2134 newspieces in 2001\n",
      "2013 newspieces in 2002\n",
      "2432 newspieces in 2003\n",
      "1982 newspieces in 2004\n",
      "2101 newspieces in 2005\n",
      "1707 newspieces in 2006\n",
      "1853 newspieces in 2007\n",
      "1746 newspieces in 2008\n",
      "1701 newspieces in 2009\n",
      "3830 newspieces in 2010\n",
      "4599 newspieces in 2011\n",
      "3885 newspieces in 2012\n",
      "157205 newspieces from 1947 to 2012\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubdate</th>\n",
       "      <th>filename</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1947-04-30</td>\n",
       "      <td>1947-04-30/102_GALE_CS119096478.txt</td>\n",
       "      <td>1947</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1947-04-30</td>\n",
       "      <td>1947-04-30/076_GALE_CS85279902.txt</td>\n",
       "      <td>1947</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1947-04-30</td>\n",
       "      <td>1947-04-30/103_GALE_CS119227550.txt</td>\n",
       "      <td>1947</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1947-04-30</td>\n",
       "      <td>1947-04-30/055_GALE_CS68502686.txt</td>\n",
       "      <td>1947</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1947-04-30</td>\n",
       "      <td>1947-04-30/117_GALE_CS134431902.txt</td>\n",
       "      <td>1947</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pubdate                             filename  year  month  week\n",
       "0  1947-04-30  1947-04-30/102_GALE_CS119096478.txt  1947      4    18\n",
       "1  1947-04-30   1947-04-30/076_GALE_CS85279902.txt  1947      4    18\n",
       "2  1947-04-30  1947-04-30/103_GALE_CS119227550.txt  1947      4    18\n",
       "3  1947-04-30   1947-04-30/055_GALE_CS68502686.txt  1947      4    18\n",
       "4  1947-04-30  1947-04-30/117_GALE_CS134431902.txt  1947      4    18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_allnews = pd.DataFrame()\n",
    "OCR_folder = 'scraping_times/data/OCRtext/'\n",
    "newscodes_folder = 'scraping_times/data/codes_for_only_News/'\n",
    "#columns = ['pubdate', 'headline', 'text', 'filename']\n",
    "columns = ['pubdate', 'filename']\n",
    "\n",
    "min_file_size = 1000 # bytes\n",
    "yearrange = range(1947,2013)\n",
    "\n",
    "for year in yearrange:\n",
    "    \n",
    "    newscodes_filenames = glob.glob(newscodes_folder + str(year) + '*')\n",
    "    filecodes_news = []\n",
    "    for infile in newscodes_filenames:\n",
    "        with open(infile) as f:\n",
    "            filecodes_news += f.read().split()\n",
    "    \n",
    "    all_infiles = glob.glob(OCR_folder + str(year) + '*/*')\n",
    "    to_df = []\n",
    "    for infile in all_infiles:\n",
    "        file_size = os.stat(infile).st_size # in bytes\n",
    "        if file_size > min_file_size:\n",
    "            \n",
    "            with open(infile) as f:\n",
    "                lines = [ i[:-1] for i in f.readlines()[2:] ]\n",
    "                headline = lines[0]\n",
    "                pubdate = infile.split('/')[3]\n",
    "                text = ' '.join(lines[1:])\n",
    "                filename = pubdate+'/'+infile.split('/')[-1]\n",
    "              \n",
    "                filecode = filename.split('_')[-1][:-4]\n",
    "                IS_NEWS = ( filecode in filecodes_news )\n",
    "                HAS_ENOUGH_TEXT = (len(text) > 50)\n",
    "                \n",
    "                if IS_NEWS and HAS_ENOUGH_TEXT:\n",
    "                    #to_df += [( pubdate, headline, text, filename )]\n",
    "                    to_df += [( pubdate, filename )]\n",
    "                    \n",
    "    print(\"%d newspieces in %d\" % (len(to_df),year))                \n",
    "    df_allnews = df_allnews.append(pd.DataFrame(to_df,columns=columns),ignore_index=True)\n",
    "\n",
    "df_allnews['year']  = df_allnews['pubdate'].apply( lambda x: int(x[:4]) )\n",
    "df_allnews['month'] = df_allnews['pubdate'].apply( lambda x: int(x[5:7]) )\n",
    "df_allnews['week']  = df_allnews['pubdate'].apply( lambda x: calc_week(x) ) \n",
    "    \n",
    "print(\"%d newspieces from %d to %d\" % (len(df_allnews), min(yearrange), max(yearrange)) )\n",
    "df_allnews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 50  # 10, 15, 20, 25, 50\n",
    "topics_per_document = pd.read_csv('times_data/document_topic_distributions_'+str(n_topics)+'topics.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate topic distributions per period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "to_date = lambda date: datetime.fromtimestamp(time.mktime(time.strptime('{} {} 1'.format(date[0],date[1]), '%Y %W %w'))) \n",
    "\n",
    "doc_to_date = { row.filename: (to_date((row.year, row.week)), row.week)\n",
    "                for _, row in df_allnews.iterrows() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = set(sorted(doc_to_date.values()))\n",
    "all_documents = topics_per_document.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1951-10-24/044_GALE_CS52514136.txt' not found\n",
      "'1954-06-16/028_GALE_CS69030608.txt' not found\n",
      "20000/157205 done\n",
      "'1963-03-13/063_GALE_CS102327405.txt' not found\n",
      "'1966-11-23/002_GALE_CS17263479.txt' not found\n",
      "40000/157205 done\n",
      "'1975-11-26/138_GALE_CS269319546.txt' not found\n",
      "60000/157205 done\n",
      "'1977-03-02/145_GALE_CS268664930.txt' not found\n",
      "'1978-05-10/142_GALE_CS268665514.txt' not found\n",
      "'1982-04-21/007_GALE_CS17926805.txt' not found\n",
      "'1982-04-21/092_GALE_CS151227029.txt' not found\n",
      "'1983-06-15/012_GALE_CS18582735.txt' not found\n",
      "80000/157205 done\n",
      "100000/157205 done\n",
      "120000/157205 done\n",
      "'2000-06-07/068_GALE_IF0501357530.txt' not found\n",
      "'2000-02-02/376_GALE_IF0501002958.txt' not found\n",
      "'2002-02-27/393_GALE_IF0501480732.txt' not found\n",
      "'2002-02-20/064_GALE_IF0501101607.txt' not found\n",
      "140000/157205 done\n"
     ]
    }
   ],
   "source": [
    "period_names = ['week','month','quarter','year']\n",
    "agg_topics = { pn:{} for pn in period_names }\n",
    "\n",
    "i=0\n",
    "for doc_id, dates in doc_to_date.items():\n",
    "    i+=1\n",
    "    if i % 20000 == 0:\n",
    "        print('{}/{} done'.format(i,len(doc_to_date)))\n",
    "        \n",
    "    year   = dates[0].year \n",
    "    quarter= (dates[0].month-1)//3\n",
    "    month  = dates[0].month\n",
    "    week   = dates[1]\n",
    "    \n",
    "    year_s    = str(year)\n",
    "    quarter_s = str(year)+'-'+str(quarter).zfill(2)\n",
    "    month_s   = str(year)+'-'+str(month).zfill(2)\n",
    "    week_s    = str(year)+'-'+str(week).zfill(2)\n",
    "    \n",
    "    try:\n",
    "        if len(topics_per_document.loc[doc_id]) < n_topics:\n",
    "            topics = topics_per_document.loc[doc_id].iloc[0].values\n",
    "        else:\n",
    "            topics = topics_per_document.loc[doc_id].values\n",
    "\n",
    "        periods = [week_s,month_s,quarter_s,year_s]\n",
    "        for period_name, period in zip(period_names, periods):\n",
    "            if period not in agg_topics[period_name]:\n",
    "                agg_topics[period_name][period]  = topics\n",
    "            else:\n",
    "                agg_topics[period_name][period] += topics\n",
    "                \n",
    "    except KeyError as e:\n",
    "        print(e, 'not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspaper = 'Times'\n",
    "period_names = ['week','month','quarter','year']\n",
    "\n",
    "for pn in period_names:\n",
    "    topics = agg_topics[pn]   \n",
    "    outfile = '../Null-models/data/aggregate_{}topics_per_{}_{}.csv'.format(n_topics,pn,\n",
    "                                                                            newspaper)\n",
    "    df = pd.DataFrame(topics.values(), index=topics.keys())\n",
    "    df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspaper = 'Times'\n",
    "period_names = ['week','month','quarter','year']\n",
    "agg_dfs = {}\n",
    "\n",
    "for pn in period_names:\n",
    "    infile = '../Null-models/data/aggregate_{}topics_per_{}_{}.csv'.format(n_topics,pn,\n",
    "                                                                           newspaper)\n",
    "    df = pd.read_csv(infile, index_col=0)\n",
    "    agg_dfs[pn] = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novelty measures\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import hamming\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def rel_entr(P, Q):                \n",
    "    return entropy(P,Q)*entropy_fix\n",
    "\n",
    "def JSD(P, Q):\n",
    "    _P = P / norm(P, ord=1)\n",
    "    _Q = Q / norm(Q, ord=1)\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (rel_entr(_P, _M) + rel_entr(_Q, _M))\n",
    "\n",
    "def BCD(P,Q):\n",
    "    _P = np.array(P / norm(P, ord=1),dtype=np.float32)\n",
    "    _Q = np.array(Q / norm(Q, ord=1),dtype=np.float32)\n",
    "    BC = np.dot(np.sqrt(_P),np.sqrt(_Q))\n",
    "    return -np.log2(BC)    \n",
    "\n",
    "def MI(_P,_Q):\n",
    "    return mutual_info_score(_P,_Q)\n",
    "\n",
    "def novelty(p,q, metric='KL'):\n",
    "    \n",
    "    if metric=='KL':\n",
    "        return rel_entr(p,q)\n",
    "    elif metric=='hamming':\n",
    "        return hamming(p>0,q>0)\n",
    "    elif metric=='euclidean':\n",
    "        return euclidean(p,q)\n",
    "    elif metric=='JSD':\n",
    "        return JSD(p,q)\n",
    "    elif metric=='BCD':\n",
    "        return BCD(p,q)\n",
    "    elif metric=='MI':\n",
    "        return MI(p,q)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "KLs = { pn:[] for pn in period_names }\n",
    "\n",
    "for pn in period_names:\n",
    "    df = agg_dfs[pn]\n",
    "    for t,tm1 in zip(df.index[1:],df.index[:-1]):\n",
    "        topics_t   = df.loc[t]/df.loc[t].sum()\n",
    "        topics_tm1 = df.loc[tm1]/df.loc[tm1].sum()\n",
    "        KL = novelty(topics_t, topics_tm1)\n",
    "        KLs[pn] += [(t,KL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing ../Null-models/data/agg_novelty_50topics_per_week_Times.csv\n",
      "Printing ../Null-models/data/agg_novelty_50topics_per_month_Times.csv\n",
      "Printing ../Null-models/data/agg_novelty_50topics_per_quarter_Times.csv\n",
      "Printing ../Null-models/data/agg_novelty_50topics_per_year_Times.csv\n"
     ]
    }
   ],
   "source": [
    "newspaper = 'Times'\n",
    "period_names = ['week','month','quarter','year']\n",
    "\n",
    "for pn in period_names:\n",
    "    outfile = '../Null-models/data/agg_novelty_{}topics_per_{}_{}.csv'.format(n_topics,pn,\n",
    "                                                                              newspaper)\n",
    "    print('Printing', outfile)\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write(''.join(['{},{}\\n'.format(p[0],p[1]) for p in KLs[pn]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
