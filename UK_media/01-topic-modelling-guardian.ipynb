{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from scipy.stats import entropy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "sns.mpl.rc(\"figure\", figsize=(9,6))\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>id</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionId</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>text</th>\n",
       "      <th>trailText</th>\n",
       "      <th>webUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fears for hostages as Algeria attacks gas comp...</td>\n",
       "      <td>world/middle-east-live/2013/jan/17/algerian-is...</td>\n",
       "      <td>2013-01-18T09:11:02Z</td>\n",
       "      <td>world</td>\n",
       "      <td>World news</td>\n",
       "      <td>We're closing this live blog but coverage will...</td>\n",
       "      <td>&lt;p&gt;• Reports of deaths of hostages and kidnapp...</td>\n",
       "      <td>https://www.theguardian.com/world/middle-east-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lance Armstrong admits doping in Oprah intervi...</td>\n",
       "      <td>sport/2013/jan/17/lance-armstrong-oprah-winfre...</td>\n",
       "      <td>2013-01-18T10:10:08Z</td>\n",
       "      <td>sport</td>\n",
       "      <td>Sport</td>\n",
       "      <td>So, what did we learn here? • That Lance Armst...</td>\n",
       "      <td>&lt;p&gt;Lance Armstrong talks to Oprah Winfrey in h...</td>\n",
       "      <td>https://www.theguardian.com/sport/2013/jan/17/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Venus Williams v Maria Sharapova – as it happened</td>\n",
       "      <td>sport/2013/jan/18/venus-williams-maria-sharapo...</td>\n",
       "      <td>2013-01-18T10:15:22Z</td>\n",
       "      <td>sport</td>\n",
       "      <td>Sport</td>\n",
       "      <td>A few words from the victor: \"Both of us were ...</td>\n",
       "      <td>&lt;p&gt;Maria Sharapova was in brutal form as she d...</td>\n",
       "      <td>https://www.theguardian.com/sport/2013/jan/18/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Fears for hostages as Algeria attacks gas comp...   \n",
       "1  Lance Armstrong admits doping in Oprah intervi...   \n",
       "2  Venus Williams v Maria Sharapova – as it happened   \n",
       "\n",
       "                                                  id               pubDate  \\\n",
       "0  world/middle-east-live/2013/jan/17/algerian-is...  2013-01-18T09:11:02Z   \n",
       "1  sport/2013/jan/17/lance-armstrong-oprah-winfre...  2013-01-18T10:10:08Z   \n",
       "2  sport/2013/jan/18/venus-williams-maria-sharapo...  2013-01-18T10:15:22Z   \n",
       "\n",
       "  sectionId sectionName                                               text  \\\n",
       "0     world  World news  We're closing this live blog but coverage will...   \n",
       "1     sport       Sport  So, what did we learn here? • That Lance Armst...   \n",
       "2     sport       Sport  A few words from the victor: \"Both of us were ...   \n",
       "\n",
       "                                           trailText  \\\n",
       "0  <p>• Reports of deaths of hostages and kidnapp...   \n",
       "1  <p>Lance Armstrong talks to Oprah Winfrey in h...   \n",
       "2  <p>Maria Sharapova was in brutal form as she d...   \n",
       "\n",
       "                                              webUrl  \n",
       "0  https://www.theguardian.com/world/middle-east-...  \n",
       "1  https://www.theguardian.com/sport/2013/jan/17/...  \n",
       "2  https://www.theguardian.com/sport/2013/jan/18/...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_allnews = pd.DataFrame()\n",
    "for year in range(2013,2019):\n",
    "    infile = 'guardian_data/%d/allnews_%d.csv' % (year, year)\n",
    "    df_year = pd.read_csv(infile, sep='\\t')\n",
    "    \n",
    "    df_year = df_year[pd.notnull(df_year['text'])]\n",
    "    df_year = df_year[df_year['text'].apply(len)>30]\n",
    "        \n",
    "    df_allnews = df_allnews.append(df_year)\n",
    "    \n",
    "df_allnews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    minlength = 3\n",
    "    \n",
    "    invalidChars = { '¡', '§', '©', '\\xad', '°', '²', '³', 'µ', '¹', '¿', '×', '\\u200b', \n",
    "                    '•', '‣', '…', '⁄', '₂', '€', '™', '▇', '■', '▶', '◆', '●', '★', '✽',\n",
    "                    '❏', '➝', '主', '原', '年', '後', '歸', '物', '舧', '舰'}\n",
    "    invalidChars = invalidChars.union(set(string.punctuation.replace(\"-\", \"–„“\")))\n",
    "    \n",
    "    for token in nltk.word_tokenize(text):\n",
    "        t = token.lower()\n",
    "        if (len(t)<minlength) or (t in stopwords) \\\n",
    "        or (t in string.punctuation) or (t[0] in string.punctuation) \\\n",
    "        or any(char in invalidChars for char in token):\n",
    "            continue\n",
    "        yield t\n",
    "        \n",
    "def normalise(vec):\n",
    "    return vec / np.dot(vec,vec)\n",
    "\n",
    "def combine_vectors(vectors):\n",
    "    return normalise(np.sum(vectors, axis=0))\n",
    "\n",
    "def important_words(vectorizer, vec, n):\n",
    "    return sorted(zip(vectorizer.get_feature_names(), vec), key=lambda x:x[1], reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's estimate how long it's going to take.\n",
    "_________________________________\n",
    "\n",
    "25 topics (time in minutes)\n",
    "[\n",
    "[100, 1.34],\n",
    "[200, 3.04],\n",
    "[500, 4.15],\n",
    "[1000, 7.44]\n",
    "]\n",
    "\n",
    "Ok, from this I get that the time in minutes is:\n",
    "T = 1.1726 + 0.00627*n_documents\n",
    "\n",
    "- 10K documents    ->  60 minutes\n",
    "- 403317 documents ->  2500 minutes (1.8 days)\n",
    "\n",
    "_________________________________\n",
    "\n",
    "50 topics (time in minutes)\n",
    "[\n",
    "[100, 2.14],\n",
    "[200, 5.46],\n",
    "[500, 8.02],\n",
    "[1000, 15.64],\n",
    "[2000, 22.82],\n",
    "]\n",
    "Ok, from this I get that the time in minutes is:\n",
    "T = 2.78 + 0.01057*n_documents\n",
    "\n",
    "- 10K documents    ->  100 minutes (2 hours)\n",
    "- 403317 documents ->  4200 minutes (3 days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403317 documents in total\n"
     ]
    }
   ],
   "source": [
    "#df = df_allnews.head(200)\n",
    "df = df_allnews\n",
    "print(len(df),\"documents in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 See the painting here Read more here\n",
      "2074 Where's Eddie? Can you spot the world's most wanted man?\n",
      "3840 Todays picture from the past is the Grand Canyon.\n",
      "5106 Share your own tributes in the comments section below\n",
      "12950 Our rolling coverage of the budget is here\n",
      "13125 Sents this year, should you be inspired ...\n",
      "22982 Stylewatch – the Guardian’s favourite outfits on the planet\n",
      "23969 Retail giant takes on Apple and Samsung\n",
      "24382 They ought to ban him for not biting hard enough\n",
      "25145 Barkers dog grooming chain is pet retailer's latest idea\n",
      "26018 The pound rises against the dollar\n",
      "26219 Watch the story of the Belarus Free Theatre in full here:\n",
      "27352 Six-year wait for retailer's financial project\n",
      "29381 Paper London Atom iPhone Cover, eBay, £29.95\n",
      "30892 There are no Guardian Australia masterclasses scheduled\n",
      "32572 From the no camp ... From the yes camp ...\n",
      "52715 More from Fashion buy of the day\n",
      "53473 Burger King $11bn Tim Hortons takeover.\n",
      "58034 It’s Thursday – sorry I’m late, chatter fans!\n",
      "60241 Bank of England talks down prospect of rate rise\n",
      "61359 Economist under fire over numbers\n",
      "62287 M25 plans part of submission to Airports Commission\n",
      "64525 Hey, wait a minute, did I miss Tuesday?\n",
      "67266 Uh-oh, here comes Monday again, and it’s cold as well!\n",
      "68364 It’s Friday! (Sorry slight delay as I’m on a train.)\n",
      "68492 It’s Wednesday - get any good gaming in last night?\n",
      "69260 It’s Friday! Let’s talk about video games all day.\n",
      "72319 I’m having one of those days...\n",
      "78157 “I’m hungry and I don’t take orders from you…”\n",
      "79592 It’s Friday, everyone. Everything is going to be OKAY.\n",
      "81125 It’s Wednesday – is it sunny where you are?\n",
      "81354 Yey, Friday! What are you up to this weekend?\n",
      "81744 It’s Monday again, but hey, at least it’s spring now!\n",
      "82052 Changing Media Summit 2015 programme\n",
      "83964 Friday! Don’t look directly at the sun!\n",
      "91920 It’s Monday – let’s talk about games to make it all better.\n",
      "92593 It’s Friday! Yay! Are any of you coming to GameCity?!\n",
      "94739 A little late due to illness : (\n",
      "96546 Slightly later than scheduled - it’s Friday!\n",
      "98601 Sorry! I am too busy pretending to be a radio presenter!\n",
      "99452 Oops sorry - I’m a bit distracted at GameCity!\n",
      "116334 Have we got it right? Let us know below.\n",
      "117943 It’s Monday and it’s raining – what could be better?!\n",
      "118323 It’s Thursday. I don’t know what happened yesterday.\n",
      "121219 This Is England ’90, 9pm Sunday on Channel 4\n",
      "121362 It’s Wednesday. Comments are now ON!\n",
      "121455 Reading on mobile? Turn your screen to landscape to view\n",
      "129740 It’s Thursday. Look at the book I found in Oxfam!\n",
      "130471 It’s Monday! How was your Christmas?\n",
      "132836 Sorry I’m late guys – I’m in LA and it’s 5am here!\n",
      "137905 The event begins at 10am PT | 1pm ET | 6pm BST\n",
      "140679 Catastrophe, 10pm Tuesday, Channel 4\n",
      "141206 It’s Call of Duty day, suckers!\n",
      "143386 It’s Thursday. Sorry, I was ill yesterday.\n",
      "144645 Click to read a larger version.\n",
      "148120 It’s quite a cold Monday morning.\n",
      "148730 It’s Friday - my son Albie’s birthday!\n",
      "159318 Halfway through the week already!\n",
      "162394 It’s Thursday everyone! Let’s chat!\n",
      "170293 It’s Mon ... no, sorry, Tuesday!\n",
      "171482 The livestream will begin at 6pm ET/11pm BST\n",
      "177301 Oh, it’s Monday. How did that happen?\n",
      "196297 Find out more about our extreme weather series here\n",
      "200173 Register for the Changing Media Summit now\n",
      "201883 ‘Tis Thursday. Let chat commence.\n",
      "218934 It’s Monday! I’m at E3 so please expect erratic service.\n",
      "219417 It’s Wednesday – and it’s June!\n",
      "220338 It’s Thursday. Better late than never?\n",
      "220791 Please refer to the PDF version here\n",
      "223318 Please refer to the PDF version here\n",
      "223337 Please refer to the PDF version here\n",
      "233926 For the full, printable PDF click here\n",
      "236462 It’s Friday! How was your Christmas?\n",
      "237007 For your overall score, please complete all the questions\n",
      "248918 Oops. I really am having a stupid week.\n",
      "258986 Support us with a monthly or a one-off contribution\n",
      "280039 Map data is from OpenStreetMap contributors\n",
      "280092 Yay, Friday! And comments are open now!\n",
      "285794 And test your knowledge of Greater Manchester’s famous food\n",
      "300216 It’s Monday. RIP George Romero.\n",
      "327752 The event described by this item has been cancelled.\n",
      "329747 The answers will be revealed Sunday, the 31st of December\n",
      "330512 For your overall score, please complete all the questions\n",
      "356797 It’s Monday. I didn’t do Friday did I?\n",
      "381714 This article was temporarily taken down on 26 March 2018.\n",
      "396385 Season 19 is available now on DVD\n"
     ]
    }
   ],
   "source": [
    "i_t = []\n",
    "\n",
    "for i,t in enumerate(df.text):\n",
    "    if len(t) < 60:\n",
    "        print(i,t)\n",
    "        i_t += [ [i,t]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "start = time.time()\n",
    "\n",
    "# list of text documents\n",
    "text = df.text.values\n",
    "doc_ids = df.id.values\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize, min_df=0.005, max_df=0.8)\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "#print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode document-term matrix\n",
    "dtm = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of document-term matrix (documents, tokens): (403317, 8727)\n",
      "Total number of tokens: 153055379\n",
      "119.43 minutes\n"
     ]
    }
   ],
   "source": [
    "# summarize encoded vector\n",
    "print('Shape of document-term matrix (documents, tokens):', dtm.shape)\n",
    "print('Total number of tokens:', dtm.sum() )\n",
    "#print(type(dtm))\n",
    "#print(dtm.toarray())\n",
    "\n",
    "end = time.time()\n",
    "print(round((end - start)/60.0,2),'minutes')\n",
    "\n",
    "save_npz('guardian_data/dtm_matrix.npz', dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = load_npz('guardian_data/dtm_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<403317x8727 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 99571826 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnz_per_row = dtm.getnnz(axis=1)\n",
    "non_null_rows = np.where(nnz_per_row > 0)[0]\n",
    "null_rows     = np.where(nnz_per_row <= 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = dtm[dtm.getnnz(1)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<403301x8727 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 99571826 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda\n",
    "\n",
    "n_topics = 50\n",
    "topic_model = lda.LDA(n_topics=n_topics, n_iter=1500, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 403301\n",
      "INFO:lda:vocab_size: 8727\n",
      "INFO:lda:n_words: 153055379\n",
      "INFO:lda:n_topics: 50\n",
      "INFO:lda:n_iter: 1500\n",
      "INFO:lda:<0> log likelihood: -1908050848\n",
      "INFO:lda:<10> log likelihood: -1583325338\n",
      "INFO:lda:<20> log likelihood: -1411131935\n",
      "INFO:lda:<30> log likelihood: -1381605888\n",
      "INFO:lda:<40> log likelihood: -1370863644\n",
      "INFO:lda:<50> log likelihood: -1365572688\n",
      "INFO:lda:<60> log likelihood: -1362869870\n",
      "INFO:lda:<70> log likelihood: -1361116591\n",
      "INFO:lda:<80> log likelihood: -1359896117\n",
      "INFO:lda:<90> log likelihood: -1358975711\n",
      "INFO:lda:<100> log likelihood: -1358101566\n",
      "INFO:lda:<110> log likelihood: -1357567030\n",
      "INFO:lda:<120> log likelihood: -1357108876\n",
      "INFO:lda:<130> log likelihood: -1356701975\n",
      "INFO:lda:<140> log likelihood: -1356284038\n",
      "INFO:lda:<150> log likelihood: -1355947002\n",
      "INFO:lda:<160> log likelihood: -1355649199\n",
      "INFO:lda:<170> log likelihood: -1355393339\n",
      "INFO:lda:<180> log likelihood: -1355132053\n",
      "INFO:lda:<190> log likelihood: -1354900571\n",
      "INFO:lda:<200> log likelihood: -1354729490\n",
      "INFO:lda:<210> log likelihood: -1354535123\n",
      "INFO:lda:<220> log likelihood: -1354335601\n",
      "INFO:lda:<230> log likelihood: -1354218437\n",
      "INFO:lda:<240> log likelihood: -1354114250\n",
      "INFO:lda:<250> log likelihood: -1353971302\n",
      "INFO:lda:<260> log likelihood: -1353878602\n",
      "INFO:lda:<270> log likelihood: -1353750528\n",
      "INFO:lda:<280> log likelihood: -1353768104\n",
      "INFO:lda:<290> log likelihood: -1353606783\n",
      "INFO:lda:<300> log likelihood: -1353554711\n",
      "INFO:lda:<310> log likelihood: -1353490039\n",
      "INFO:lda:<320> log likelihood: -1353444839\n",
      "INFO:lda:<330> log likelihood: -1353373321\n",
      "INFO:lda:<340> log likelihood: -1353290840\n",
      "INFO:lda:<350> log likelihood: -1353238366\n",
      "INFO:lda:<360> log likelihood: -1353175551\n",
      "INFO:lda:<370> log likelihood: -1353180281\n",
      "INFO:lda:<380> log likelihood: -1353059172\n",
      "INFO:lda:<390> log likelihood: -1353054078\n",
      "INFO:lda:<400> log likelihood: -1353009416\n",
      "INFO:lda:<410> log likelihood: -1352970123\n",
      "INFO:lda:<420> log likelihood: -1352930710\n",
      "INFO:lda:<430> log likelihood: -1352932037\n",
      "INFO:lda:<440> log likelihood: -1352884695\n",
      "INFO:lda:<450> log likelihood: -1352808912\n",
      "INFO:lda:<460> log likelihood: -1352730306\n",
      "INFO:lda:<470> log likelihood: -1352744775\n",
      "INFO:lda:<480> log likelihood: -1352741774\n",
      "INFO:lda:<490> log likelihood: -1352700200\n",
      "INFO:lda:<500> log likelihood: -1352650561\n",
      "INFO:lda:<510> log likelihood: -1352681740\n",
      "INFO:lda:<520> log likelihood: -1352641346\n",
      "INFO:lda:<530> log likelihood: -1352648897\n",
      "INFO:lda:<540> log likelihood: -1352565138\n",
      "INFO:lda:<550> log likelihood: -1352567222\n",
      "INFO:lda:<560> log likelihood: -1352547230\n",
      "INFO:lda:<570> log likelihood: -1352510487\n",
      "INFO:lda:<580> log likelihood: -1352465918\n",
      "INFO:lda:<590> log likelihood: -1352429457\n",
      "INFO:lda:<600> log likelihood: -1352369597\n",
      "INFO:lda:<610> log likelihood: -1352401064\n",
      "INFO:lda:<620> log likelihood: -1352339564\n",
      "INFO:lda:<630> log likelihood: -1352317915\n",
      "INFO:lda:<640> log likelihood: -1352262941\n",
      "INFO:lda:<650> log likelihood: -1352279277\n",
      "INFO:lda:<660> log likelihood: -1352253501\n",
      "INFO:lda:<670> log likelihood: -1352213165\n",
      "INFO:lda:<680> log likelihood: -1352185060\n",
      "INFO:lda:<690> log likelihood: -1352181107\n",
      "INFO:lda:<700> log likelihood: -1352079495\n",
      "INFO:lda:<710> log likelihood: -1352071239\n",
      "INFO:lda:<720> log likelihood: -1352051509\n",
      "INFO:lda:<730> log likelihood: -1351994914\n",
      "INFO:lda:<740> log likelihood: -1351978673\n",
      "INFO:lda:<750> log likelihood: -1351976504\n",
      "INFO:lda:<760> log likelihood: -1351932980\n",
      "INFO:lda:<770> log likelihood: -1351894033\n",
      "INFO:lda:<780> log likelihood: -1351846614\n",
      "INFO:lda:<790> log likelihood: -1351798542\n",
      "INFO:lda:<800> log likelihood: -1351630779\n",
      "INFO:lda:<810> log likelihood: -1351647073\n",
      "INFO:lda:<820> log likelihood: -1351614454\n",
      "INFO:lda:<830> log likelihood: -1351541850\n",
      "INFO:lda:<840> log likelihood: -1351422358\n",
      "INFO:lda:<850> log likelihood: -1351416538\n",
      "INFO:lda:<860> log likelihood: -1351341988\n",
      "INFO:lda:<870> log likelihood: -1351299911\n",
      "INFO:lda:<880> log likelihood: -1351289863\n",
      "INFO:lda:<890> log likelihood: -1351239699\n",
      "INFO:lda:<900> log likelihood: -1351217394\n",
      "INFO:lda:<910> log likelihood: -1351236516\n",
      "INFO:lda:<920> log likelihood: -1351145935\n",
      "INFO:lda:<930> log likelihood: -1351181862\n",
      "INFO:lda:<940> log likelihood: -1351119406\n",
      "INFO:lda:<950> log likelihood: -1351102637\n",
      "INFO:lda:<960> log likelihood: -1351084865\n",
      "INFO:lda:<970> log likelihood: -1351078222\n",
      "INFO:lda:<980> log likelihood: -1351054191\n",
      "INFO:lda:<990> log likelihood: -1351045373\n",
      "INFO:lda:<1000> log likelihood: -1351122783\n",
      "INFO:lda:<1010> log likelihood: -1351124424\n",
      "INFO:lda:<1020> log likelihood: -1351073933\n",
      "INFO:lda:<1030> log likelihood: -1351137611\n",
      "INFO:lda:<1040> log likelihood: -1351069443\n",
      "INFO:lda:<1050> log likelihood: -1351078571\n",
      "INFO:lda:<1060> log likelihood: -1351063792\n",
      "INFO:lda:<1070> log likelihood: -1351001434\n",
      "INFO:lda:<1080> log likelihood: -1351001080\n",
      "INFO:lda:<1090> log likelihood: -1350966792\n",
      "INFO:lda:<1100> log likelihood: -1350990041\n",
      "INFO:lda:<1110> log likelihood: -1350933756\n",
      "INFO:lda:<1120> log likelihood: -1350949791\n",
      "INFO:lda:<1130> log likelihood: -1350948865\n",
      "INFO:lda:<1140> log likelihood: -1350924851\n",
      "INFO:lda:<1150> log likelihood: -1350903277\n",
      "INFO:lda:<1160> log likelihood: -1350881476\n",
      "INFO:lda:<1170> log likelihood: -1350881235\n",
      "INFO:lda:<1180> log likelihood: -1350887148\n",
      "INFO:lda:<1190> log likelihood: -1350899486\n",
      "INFO:lda:<1200> log likelihood: -1350908032\n",
      "INFO:lda:<1210> log likelihood: -1350912316\n",
      "INFO:lda:<1220> log likelihood: -1350905295\n",
      "INFO:lda:<1230> log likelihood: -1350920480\n",
      "INFO:lda:<1240> log likelihood: -1350865670\n",
      "INFO:lda:<1250> log likelihood: -1350907743\n",
      "INFO:lda:<1260> log likelihood: -1350858872\n",
      "INFO:lda:<1270> log likelihood: -1350881659\n",
      "INFO:lda:<1280> log likelihood: -1350899801\n",
      "INFO:lda:<1290> log likelihood: -1350941930\n",
      "INFO:lda:<1300> log likelihood: -1350955807\n",
      "INFO:lda:<1310> log likelihood: -1350877502\n",
      "INFO:lda:<1320> log likelihood: -1350899399\n",
      "INFO:lda:<1330> log likelihood: -1350867590\n",
      "INFO:lda:<1340> log likelihood: -1350872344\n",
      "INFO:lda:<1350> log likelihood: -1350882272\n",
      "INFO:lda:<1360> log likelihood: -1350876101\n",
      "INFO:lda:<1370> log likelihood: -1350861276\n",
      "INFO:lda:<1380> log likelihood: -1350861916\n",
      "INFO:lda:<1390> log likelihood: -1350904596\n",
      "INFO:lda:<1400> log likelihood: -1350882250\n",
      "INFO:lda:<1410> log likelihood: -1350903739\n",
      "INFO:lda:<1420> log likelihood: -1350925940\n",
      "INFO:lda:<1430> log likelihood: -1350960442\n",
      "INFO:lda:<1440> log likelihood: -1350932695\n",
      "INFO:lda:<1450> log likelihood: -1350917231\n",
      "INFO:lda:<1460> log likelihood: -1350882834\n",
      "INFO:lda:<1470> log likelihood: -1350902559\n",
      "INFO:lda:<1480> log likelihood: -1350905732\n",
      "INFO:lda:<1490> log likelihood: -1350883826\n",
      "INFO:lda:<1499> log likelihood: -1350919674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1637.52 minutes\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "document_topic_distributions = topic_model.fit_transform(dtm)\n",
    "\n",
    "end = time.time()\n",
    "print(round((end - start)/60.0,2),'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "topic_names = ['Topic %d'%k for k in range(1, n_topics + 1)]\n",
    "\n",
    "topic_word_distributions = pd.DataFrame(topic_model.components_, columns=vocab, index=topic_names)\n",
    "\n",
    "document_topic_distributions = pd.DataFrame(document_topic_distributions,\n",
    "                                            columns=topic_names,\n",
    "                                            index=doc_ids[non_null_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_distributions.to_csv('guardian_data/document_topic_distributions_'+str(n_topics)+'topics.csv')\n",
    "topic_word_distributions.to_csv('guardian_data/topic_word_distributions_'+str(n_topics)+'topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "growth      0.012292\n",
       "economy     0.009615\n",
       "bank        0.009512\n",
       "year        0.008162\n",
       "market      0.007252\n",
       "markets     0.006865\n",
       "said        0.006764\n",
       "rate        0.006681\n",
       "greece      0.006339\n",
       "also        0.006335\n",
       "economic    0.006279\n",
       "last        0.006176\n",
       "rates       0.005592\n",
       "since       0.005583\n",
       "prices      0.005391\n",
       "Name: Topic 1, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.loc['Topic 1'].sort_values(ascending=False).head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
