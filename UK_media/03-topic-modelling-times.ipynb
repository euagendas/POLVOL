{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03-topic-modelling-times\n",
    "\n",
    "Calculate the topic distribution for every article in every week, over a range of years.\n",
    "So first I'll put all the documents together, then calculate the words-per-topic matrix, then run a long model, and finally produce the documents-per-topic matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import string\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from scipy.stats import entropy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "sns.mpl.rc(\"figure\", figsize=(9,6))\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "all_article_types = pd.read_csv('scraping_times/data/all_article_types.csv')\n",
    "\n",
    "all_article_types = all_article_types.set_index([\n",
    "                         [ x[:10] for x in all_article_types.filename ],\n",
    "                         all_article_types.filename])\n",
    "\n",
    "\n",
    "df_allnews = pd.DataFrame()\n",
    "OCR_folder = 'scraping_times/data/OCRtext/'\n",
    "columns = ['pubdate', 'headline', 'text', 'id', 'type_article']\n",
    "\n",
    "min_file_size = 1000 # bytes\n",
    "yearrange = range(1947,1950)\n",
    "\n",
    "for year in yearrange:\n",
    "    all_infiles = glob.glob(OCR_folder + str(year) + '*/*')\n",
    "    to_df = []\n",
    "    for infile in all_infiles:\n",
    "        file_size = os.stat(infile).st_size # in bytes\n",
    "        if file_size > min_file_size:\n",
    "            \n",
    "            with open(infile) as f:\n",
    "                lines = [ i[:-1] for i in f.readlines()[2:] ]\n",
    "                headline = lines[0]\n",
    "                pubdate = infile.split('/')[3]\n",
    "                text = ' '.join(lines[1:])\n",
    "                filename = pubdate+'/'+infile.split('/')[-1]\n",
    "                type_article = all_article_types.loc[pubdate].loc[filename]['type_article']\n",
    "              \n",
    "                if len(text) > 50:\n",
    "                    to_df += [( pubdate, headline, text, filename, type_article )]\n",
    "                    \n",
    "    print(\"%d newspieces in %d\" % (len(to_df),year))                \n",
    "    df_allnews = df_allnews.append(pd.DataFrame(to_df,columns=columns),ignore_index=True)\n",
    "    \n",
    "print(\"%d newspieces from %d to %d\" % (len(df_allnews), min(yearrange), max(yearrange)) )\n",
    "df_allnews.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4161 newspieces in 1947\n",
      "3290 newspieces in 1948\n",
      "4342 newspieces in 1949\n",
      "4535 newspieces in 1950\n",
      "4664 newspieces in 1951\n",
      "4989 newspieces in 1952\n",
      "5669 newspieces in 1953\n",
      "5286 newspieces in 1954\n",
      "5375 newspieces in 1955\n",
      "5305 newspieces in 1956\n",
      "5378 newspieces in 1957\n",
      "5857 newspieces in 1958\n",
      "5975 newspieces in 1959\n",
      "6435 newspieces in 1960\n",
      "7221 newspieces in 1961\n",
      "7726 newspieces in 1962\n",
      "7519 newspieces in 1963\n",
      "7493 newspieces in 1964\n",
      "7389 newspieces in 1965\n",
      "6642 newspieces in 1966\n",
      "6914 newspieces in 1967\n",
      "7461 newspieces in 1968\n",
      "9087 newspieces in 1969\n",
      "7945 newspieces in 1970\n",
      "7662 newspieces in 1971\n",
      "7790 newspieces in 1972\n",
      "7721 newspieces in 1973\n",
      "7378 newspieces in 1974\n",
      "7459 newspieces in 1975\n",
      "7766 newspieces in 1976\n",
      "9218 newspieces in 1977\n",
      "7072 newspieces in 1978\n",
      "901 newspieces in 1979\n",
      "6896 newspieces in 1980\n",
      "6764 newspieces in 1981\n",
      "6260 newspieces in 1982\n",
      "6593 newspieces in 1983\n",
      "5781 newspieces in 1984\n",
      "6374 newspieces in 1985\n",
      "7212 newspieces in 1986\n",
      "8067 newspieces in 1987\n",
      "7961 newspieces in 1988\n",
      "7934 newspieces in 1989\n",
      "7635 newspieces in 1990\n",
      "4889 newspieces in 1991\n",
      "7343 newspieces in 1992\n",
      "5154 newspieces in 1993\n",
      "7537 newspieces in 1994\n",
      "5413 newspieces in 1995\n",
      "8442 newspieces in 1996\n",
      "9245 newspieces in 1997\n",
      "8496 newspieces in 1998\n",
      "6714 newspieces in 1999\n",
      "11323 newspieces in 2000\n",
      "7608 newspieces in 2001\n",
      "8162 newspieces in 2002\n",
      "8715 newspieces in 2003\n",
      "8592 newspieces in 2004\n",
      "8873 newspieces in 2005\n",
      "8376 newspieces in 2006\n",
      "7995 newspieces in 2007\n",
      "7257 newspieces in 2008\n",
      "7517 newspieces in 2009\n",
      "7296 newspieces in 2010\n",
      "7142 newspieces in 2011\n",
      "6158 newspieces in 2012\n",
      "455349 newspieces from 1947 to 2012\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubdate</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1947-04-30</td>\n",
       "      <td>Two Minutes</td>\n",
       "      <td>The thought that there is a list of 450,000 pe...</td>\n",
       "      <td>1947-04-30/068_GALE_CS84624971.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1947-04-30</td>\n",
       "      <td>&amp;USTRALIAN HEAT WAVE</td>\n",
       "      <td>-DEATHS HIGHER TEMPERATURES FORECAST FROM OUR ...</td>\n",
       "      <td>1947-04-30/143_GALE_CS135618086.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1947-04-30</td>\n",
       "      <td>4366,000,000 SAVINGS</td>\n",
       "      <td>TARGET LORD MONTGOMERY ON WINNING PROSPERITY F...</td>\n",
       "      <td>1947-04-30/102_GALE_CS119096478.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1947-04-30</td>\n",
       "      <td>FINANCIAL TALKS WrM</td>\n",
       "      <td>- PAKISTAN RELEASE OF BALANCES Financial negot...</td>\n",
       "      <td>1947-04-30/037_GALE_CS36391090.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1947-04-30</td>\n",
       "      <td>FATEFUL YEAR FOR CEYLON</td>\n",
       "      <td>SELF-GOVERNMENT UNDER THE SOULBURY SCHEME A LA...</td>\n",
       "      <td>1947-04-30/076_GALE_CS85279902.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pubdate                 headline  \\\n",
       "0  1947-04-30              Two Minutes   \n",
       "1  1947-04-30     &USTRALIAN HEAT WAVE   \n",
       "2  1947-04-30     4366,000,000 SAVINGS   \n",
       "3  1947-04-30      FINANCIAL TALKS WrM   \n",
       "4  1947-04-30  FATEFUL YEAR FOR CEYLON   \n",
       "\n",
       "                                                text  \\\n",
       "0  The thought that there is a list of 450,000 pe...   \n",
       "1  -DEATHS HIGHER TEMPERATURES FORECAST FROM OUR ...   \n",
       "2  TARGET LORD MONTGOMERY ON WINNING PROSPERITY F...   \n",
       "3  - PAKISTAN RELEASE OF BALANCES Financial negot...   \n",
       "4  SELF-GOVERNMENT UNDER THE SOULBURY SCHEME A LA...   \n",
       "\n",
       "                                    id  \n",
       "0   1947-04-30/068_GALE_CS84624971.txt  \n",
       "1  1947-04-30/143_GALE_CS135618086.txt  \n",
       "2  1947-04-30/102_GALE_CS119096478.txt  \n",
       "3   1947-04-30/037_GALE_CS36391090.txt  \n",
       "4   1947-04-30/076_GALE_CS85279902.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_allnews = pd.DataFrame()\n",
    "OCR_folder = 'scraping_times/data/OCRtext/'\n",
    "columns = ['pubdate', 'headline', 'text', 'id']\n",
    "\n",
    "min_file_size = 1000 # bytes\n",
    "yearrange = range(1947,2013)\n",
    "\n",
    "for year in yearrange:\n",
    "    all_infiles = glob.glob(OCR_folder + str(year) + '*/*')\n",
    "    to_df = []\n",
    "    for infile in all_infiles:\n",
    "        file_size = os.stat(infile).st_size # in bytes\n",
    "        if file_size > min_file_size:\n",
    "            \n",
    "            with open(infile) as f:\n",
    "                lines = [ i[:-1] for i in f.readlines()[2:] ]\n",
    "                headline = lines[0]\n",
    "                pubdate = infile.split('/')[3]\n",
    "                text = ' '.join(lines[1:])\n",
    "                filename = pubdate+'/'+infile.split('/')[-1]\n",
    "              \n",
    "                if len(text) > 50:\n",
    "                    to_df += [( pubdate, headline, text, filename )]\n",
    "                    \n",
    "    print(\"%d newspieces in %d\" % (len(to_df),year))                \n",
    "    df_allnews = df_allnews.append(pd.DataFrame(to_df,columns=columns),ignore_index=True)\n",
    "    \n",
    "print(\"%d newspieces from %d to %d\" % (len(df_allnews), min(yearrange), max(yearrange)) )\n",
    "df_allnews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some ads are still classified as news:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['2011-07-20', 'Beauty and the beach: ten great products',\n",
       "       'Murad sunblock Invest in quality sunscreen to prevent premature ageing and, of course, sunburn! Murad pomphenol sunguard dietary supplement Protect your skin from the inside with this clever pomegranate supplement COWSHED i..;|-jnS UwJy \\'it\\'ttoo Mop basil mint shampoo Give your hair a holiday: wash out impurities caused by pollution and styling products with this fresh shampoo Mop C-System moisture complex for conditioning and detangling Sun and chlorine can dry out your hair, so give it a treat with a rich conditioner t JF3U m Cowshed lazy cow soothing body lotion A luxurious- smelling lotion that\\'s perfect for calming sun-exposed skin »«((*»» wgs-\\'il; piMMigt* mop- St Tropez wash-off instant shimmer stick Ifthe weather lets you down, create a tan with this handy instant bronzer A? \\'-A? - > A * s4AAk \" \"* ^ ^ ^y? » » <■ * A *x, fi i. * f f m A W\\'l s ^=^^ . Bellapierre compact foundation Choose a compact a little darker than your real skin tone to control shine A Transformulas FirmSlim anti-cellulite mousse Use morning and evening to help tone the skin i O.RI.nail lacquer Choose barely-there nude and pink shades for natural nails on the beach s OvPyl NAIL ucQvm * ftp*** Q- F - J ««L-0,5ttCM2 Rose & Co Cherry Kiss lipbalm All that sea air could leave lips cracked, so keep them luscious with a tasty balm X',\n",
       "       '2011-07-20/399_GALE_IF0504252180.txt'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Some ads are still classified as news:\")\n",
    "df_allnews[df_allnews.pubdate=='2011-07-20'].iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def has_letter(s):\n",
    "    return ( re.search('[a-zA-Z]', s) != None )\n",
    "\n",
    "def tokenize(text0):\n",
    "    #stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "    stopwords = STOP_WORDS\n",
    "    for word in ['year','new','said']:\n",
    "        stopwords.add(word)\n",
    "    \n",
    "    minlength = 3\n",
    "    \n",
    "    invalidChars = { '¡', '§', '©', '\\xad', '°', '²', '³', 'µ', '¹', '¿', '×', '\\u200b', \n",
    "                    '•', '‣', '…', '⁄', '₂', '€', '™', '▇', '■', '▶', '◆', '●', '★', '✽',\n",
    "                    '❏', '➝', '主', '原', '年', '後', '歸', '物', '舧', '舰'}\n",
    "    invalidChars = invalidChars.union(set(string.punctuation.replace(\"-\", \"–„“\")))\n",
    "    \n",
    "    text = text0.replace('- ','') # for split words\n",
    "    \n",
    "    for token in nltk.word_tokenize(text):\n",
    "        t = token.lower()\n",
    "        if (len(t)<minlength) or (t in stopwords) \\\n",
    "        or (t in string.punctuation) or (t[0] in string.punctuation) \\\n",
    "        or any(char in invalidChars for char in token) \\\n",
    "        or not has_letter(t):\n",
    "            continue\n",
    "        yield t\n",
    "        \n",
    "def normalise(vec):\n",
    "    return vec / np.dot(vec,vec)\n",
    "\n",
    "def combine_vectors(vectors):\n",
    "    return normalise(np.sum(vectors, axis=0))\n",
    "\n",
    "def important_words(vectorizer, vec, n):\n",
    "    return sorted(zip(vectorizer.get_feature_names(), vec), key=lambda x:x[1], reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listed documents\n",
      "Created the transform\n",
      "Ran vectorizer.fit(text)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "start = time.time()\n",
    "\n",
    "# list of text documents\n",
    "text    = df_allnews.text.values\n",
    "doc_ids = df_allnews.id.values\n",
    "\n",
    "print(\"Listed documents\")\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize, min_df=0.005, max_df=0.8)\n",
    "\n",
    "print(\"Created the transform\")\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "print(\"Ran vectorizer.fit(text)\")\n",
    "\n",
    "# summarize\n",
    "#print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode document-term matrix\n",
    "dtm = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of document-term matrix (documents, tokens): (455349, 6033)\n",
      "Total number of tokens: 85821427\n",
      "103.07 minutes\n"
     ]
    }
   ],
   "source": [
    "# summarize encoded vector\n",
    "print('Shape of document-term matrix (documents, tokens):', dtm.shape)\n",
    "print('Total number of tokens:', dtm.sum() )\n",
    "#print(type(dtm))\n",
    "#print(dtm.toarray())\n",
    "\n",
    "end = time.time()\n",
    "print(round((end - start)/60.0,2),'minutes')\n",
    "\n",
    "from scipy.sparse import save_npz\n",
    "save_npz('times_data/dtm_matrix.npz', dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<455349x6033 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 57260404 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import load_npz\n",
    "dtm = load_npz('times_data/dtm_matrix.npz')\n",
    "\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<455282x6033 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 57260404 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnz_per_row = dtm.getnnz(axis=1)\n",
    "non_null_rows = np.where(nnz_per_row > 0)[0]\n",
    "null_rows     = np.where(nnz_per_row <= 0)[0]\n",
    "\n",
    "dtm = dtm[dtm.getnnz(1)>0]\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda\n",
    "\n",
    "#n_topics = 30\n",
    "#n_topics = 50\n",
    "#n_topics = 70\n",
    "\n",
    "n_topics = 25\n",
    "\n",
    "topic_model = lda.LDA(n_topics=n_topics, n_iter=1500, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 455282\n",
      "INFO:lda:vocab_size: 6033\n",
      "INFO:lda:n_words: 85821427\n",
      "INFO:lda:n_topics: 25\n",
      "INFO:lda:n_iter: 1500\n",
      "INFO:lda:<0> log likelihood: -999387143\n",
      "INFO:lda:<10> log likelihood: -802106738\n",
      "INFO:lda:<20> log likelihood: -727036001\n",
      "INFO:lda:<30> log likelihood: -714965086\n",
      "INFO:lda:<40> log likelihood: -711041390\n",
      "INFO:lda:<50> log likelihood: -709297409\n",
      "INFO:lda:<60> log likelihood: -708421660\n",
      "INFO:lda:<70> log likelihood: -708009523\n",
      "INFO:lda:<80> log likelihood: -707688464\n",
      "INFO:lda:<90> log likelihood: -707479369\n",
      "INFO:lda:<100> log likelihood: -707321404\n",
      "INFO:lda:<110> log likelihood: -707195616\n",
      "INFO:lda:<120> log likelihood: -707108559\n",
      "INFO:lda:<130> log likelihood: -707024875\n",
      "INFO:lda:<140> log likelihood: -706934412\n",
      "INFO:lda:<150> log likelihood: -706875198\n",
      "INFO:lda:<160> log likelihood: -706822219\n",
      "INFO:lda:<170> log likelihood: -706759149\n",
      "INFO:lda:<180> log likelihood: -706709297\n",
      "INFO:lda:<190> log likelihood: -706702517\n",
      "INFO:lda:<200> log likelihood: -706655422\n",
      "INFO:lda:<210> log likelihood: -706604155\n",
      "INFO:lda:<220> log likelihood: -706558066\n",
      "INFO:lda:<230> log likelihood: -706528707\n",
      "INFO:lda:<240> log likelihood: -706517064\n",
      "INFO:lda:<250> log likelihood: -706459130\n",
      "INFO:lda:<260> log likelihood: -706466339\n",
      "INFO:lda:<270> log likelihood: -706403305\n",
      "INFO:lda:<280> log likelihood: -706363074\n",
      "INFO:lda:<290> log likelihood: -706362947\n",
      "INFO:lda:<300> log likelihood: -706316982\n",
      "INFO:lda:<310> log likelihood: -706281851\n",
      "INFO:lda:<320> log likelihood: -706218422\n",
      "INFO:lda:<330> log likelihood: -706184185\n",
      "INFO:lda:<340> log likelihood: -706119524\n",
      "INFO:lda:<350> log likelihood: -706087626\n",
      "INFO:lda:<360> log likelihood: -706070217\n",
      "INFO:lda:<370> log likelihood: -706041806\n",
      "INFO:lda:<380> log likelihood: -705988872\n",
      "INFO:lda:<390> log likelihood: -705936202\n",
      "INFO:lda:<400> log likelihood: -705905378\n",
      "INFO:lda:<410> log likelihood: -705862068\n",
      "INFO:lda:<420> log likelihood: -705871085\n",
      "INFO:lda:<430> log likelihood: -705849325\n",
      "INFO:lda:<440> log likelihood: -705825217\n",
      "INFO:lda:<450> log likelihood: -705761444\n",
      "INFO:lda:<460> log likelihood: -705729984\n",
      "INFO:lda:<470> log likelihood: -705700522\n",
      "INFO:lda:<480> log likelihood: -705668777\n",
      "INFO:lda:<490> log likelihood: -705654471\n",
      "INFO:lda:<500> log likelihood: -705582486\n",
      "INFO:lda:<510> log likelihood: -705562881\n",
      "INFO:lda:<520> log likelihood: -705590449\n",
      "INFO:lda:<530> log likelihood: -705549151\n",
      "INFO:lda:<540> log likelihood: -705526142\n",
      "INFO:lda:<550> log likelihood: -705508959\n",
      "INFO:lda:<560> log likelihood: -705481255\n",
      "INFO:lda:<570> log likelihood: -705488608\n",
      "INFO:lda:<580> log likelihood: -705456378\n",
      "INFO:lda:<590> log likelihood: -705435761\n",
      "INFO:lda:<600> log likelihood: -705433551\n",
      "INFO:lda:<610> log likelihood: -705448571\n",
      "INFO:lda:<620> log likelihood: -705416715\n",
      "INFO:lda:<630> log likelihood: -705407018\n",
      "INFO:lda:<640> log likelihood: -705401797\n",
      "INFO:lda:<650> log likelihood: -705405606\n",
      "INFO:lda:<660> log likelihood: -705408727\n",
      "INFO:lda:<670> log likelihood: -705390172\n",
      "INFO:lda:<680> log likelihood: -705387522\n",
      "INFO:lda:<690> log likelihood: -705401869\n",
      "INFO:lda:<700> log likelihood: -705383488\n",
      "INFO:lda:<710> log likelihood: -705399310\n",
      "INFO:lda:<720> log likelihood: -705378488\n",
      "INFO:lda:<730> log likelihood: -705387176\n",
      "INFO:lda:<740> log likelihood: -705384550\n",
      "INFO:lda:<750> log likelihood: -705409407\n",
      "INFO:lda:<760> log likelihood: -705414894\n",
      "INFO:lda:<770> log likelihood: -705405871\n",
      "INFO:lda:<780> log likelihood: -705393665\n",
      "INFO:lda:<790> log likelihood: -705389660\n",
      "INFO:lda:<800> log likelihood: -705361330\n",
      "INFO:lda:<810> log likelihood: -705399469\n",
      "INFO:lda:<820> log likelihood: -705399458\n",
      "INFO:lda:<830> log likelihood: -705415314\n",
      "INFO:lda:<840> log likelihood: -705396993\n",
      "INFO:lda:<850> log likelihood: -705397132\n",
      "INFO:lda:<860> log likelihood: -705430865\n",
      "INFO:lda:<870> log likelihood: -705401092\n",
      "INFO:lda:<880> log likelihood: -705433754\n",
      "INFO:lda:<890> log likelihood: -705433098\n",
      "INFO:lda:<900> log likelihood: -705406175\n",
      "INFO:lda:<910> log likelihood: -705444155\n",
      "INFO:lda:<920> log likelihood: -705427488\n",
      "INFO:lda:<930> log likelihood: -705445965\n",
      "INFO:lda:<940> log likelihood: -705456742\n",
      "INFO:lda:<950> log likelihood: -705450749\n",
      "INFO:lda:<960> log likelihood: -705449837\n",
      "INFO:lda:<970> log likelihood: -705430206\n",
      "INFO:lda:<980> log likelihood: -705431336\n",
      "INFO:lda:<990> log likelihood: -705472605\n",
      "INFO:lda:<1000> log likelihood: -705463715\n",
      "INFO:lda:<1010> log likelihood: -705460708\n",
      "INFO:lda:<1020> log likelihood: -705461279\n",
      "INFO:lda:<1030> log likelihood: -705463292\n",
      "INFO:lda:<1040> log likelihood: -705441998\n",
      "INFO:lda:<1050> log likelihood: -705448820\n",
      "INFO:lda:<1060> log likelihood: -705411345\n",
      "INFO:lda:<1070> log likelihood: -705390738\n",
      "INFO:lda:<1080> log likelihood: -705446597\n",
      "INFO:lda:<1090> log likelihood: -705454900\n",
      "INFO:lda:<1100> log likelihood: -705427656\n",
      "INFO:lda:<1110> log likelihood: -705432058\n",
      "INFO:lda:<1120> log likelihood: -705444467\n",
      "INFO:lda:<1130> log likelihood: -705475884\n",
      "INFO:lda:<1140> log likelihood: -705452017\n",
      "INFO:lda:<1150> log likelihood: -705477092\n",
      "INFO:lda:<1160> log likelihood: -705459000\n",
      "INFO:lda:<1170> log likelihood: -705448414\n",
      "INFO:lda:<1180> log likelihood: -705453295\n",
      "INFO:lda:<1190> log likelihood: -705447072\n",
      "INFO:lda:<1200> log likelihood: -705476906\n",
      "INFO:lda:<1210> log likelihood: -705482936\n",
      "INFO:lda:<1220> log likelihood: -705454624\n",
      "INFO:lda:<1230> log likelihood: -705467271\n",
      "INFO:lda:<1240> log likelihood: -705458579\n",
      "INFO:lda:<1250> log likelihood: -705457857\n",
      "INFO:lda:<1260> log likelihood: -705465538\n",
      "INFO:lda:<1270> log likelihood: -705443314\n",
      "INFO:lda:<1280> log likelihood: -705469114\n",
      "INFO:lda:<1290> log likelihood: -705459894\n",
      "INFO:lda:<1300> log likelihood: -705508032\n",
      "INFO:lda:<1310> log likelihood: -705502896\n",
      "INFO:lda:<1320> log likelihood: -705462137\n",
      "INFO:lda:<1330> log likelihood: -705451345\n",
      "INFO:lda:<1340> log likelihood: -705447967\n",
      "INFO:lda:<1350> log likelihood: -705452771\n",
      "INFO:lda:<1360> log likelihood: -705471136\n",
      "INFO:lda:<1370> log likelihood: -705468589\n",
      "INFO:lda:<1380> log likelihood: -705488140\n",
      "INFO:lda:<1390> log likelihood: -705488028\n",
      "INFO:lda:<1400> log likelihood: -705474082\n",
      "INFO:lda:<1410> log likelihood: -705480026\n",
      "INFO:lda:<1420> log likelihood: -705460646\n",
      "INFO:lda:<1430> log likelihood: -705461640\n",
      "INFO:lda:<1440> log likelihood: -705473116\n",
      "INFO:lda:<1450> log likelihood: -705493691\n",
      "INFO:lda:<1460> log likelihood: -705492271\n",
      "INFO:lda:<1470> log likelihood: -705473633\n",
      "INFO:lda:<1480> log likelihood: -705500095\n",
      "INFO:lda:<1490> log likelihood: -705444064\n",
      "INFO:lda:<1499> log likelihood: -705458980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556.6239311854044 minutes\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "document_topic_distributions = topic_model.fit_transform(dtm)\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60.0,'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "topic_names = ['Topic %d'%k for k in range(1, n_topics + 1)]\n",
    "\n",
    "topic_word_distributions = pd.DataFrame(topic_model.components_, columns=vocab, index=topic_names)\n",
    "\n",
    "document_topic_distributions = pd.DataFrame(document_topic_distributions,\n",
    "                                            columns=topic_names,\n",
    "                                            index=doc_ids[non_null_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_distributions.to_csv('times_data/document_topic_distributions_'+str(n_topics)+'topics.csv')\n",
    "topic_word_distributions.to_csv('times_data/topic_word_distributions_'+str(n_topics)+'topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "court       0.029122\n",
       "justice     0.017137\n",
       "case        0.015390\n",
       "lord        0.013637\n",
       "law         0.013350\n",
       "appeal      0.012037\n",
       "act         0.011992\n",
       "judge       0.008661\n",
       "order       0.008224\n",
       "section     0.007727\n",
       "legal       0.007448\n",
       "evidence    0.006885\n",
       "judgment    0.005728\n",
       "decision    0.005381\n",
       "right       0.004980\n",
       "Name: Topic 2, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.loc['Topic 2'].sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "war           0.011055\n",
       "president     0.010404\n",
       "military      0.009081\n",
       "soviet        0.008849\n",
       "forces        0.007517\n",
       "defence       0.006984\n",
       "army          0.006935\n",
       "american      0.006931\n",
       "united        0.006644\n",
       "government    0.006548\n",
       "israel        0.005824\n",
       "troops        0.005445\n",
       "states        0.005442\n",
       "foreign       0.005119\n",
       "peace         0.004751\n",
       "Name: Topic 3, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.loc['Topic 3'].sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_KLs_50topics.csv\r\n",
      "all_KLs_per_year_50topics.csv\r\n",
      "dates.txt\r\n",
      "date_to_entropies.csv\r\n",
      "date_to_n_issue.csv\r\n",
      "document_topic_distributions_10topics.csv\r\n",
      "document_topic_distributions_15topics.csv\r\n",
      "document_topic_distributions_20topics.csv\r\n",
      "document_topic_distributions_25topics.csv\r\n",
      "document_topic_distributions_50topics.csv\r\n",
      "dtm_matrix.npz\r\n",
      "eff_n_topics_per_month.csv\r\n",
      "eff_n_topics_per_trimester.csv\r\n",
      "eff_n_topics_per_week.csv\r\n",
      "eff_n_topics_per_year.csv\r\n",
      "newspieces_per_month.csv\r\n",
      "newspieces_per_trimester.csv\r\n",
      "newspieces_per_year.csv\r\n",
      "\u001b[0m\u001b[01;31mnovelty_50topics_times.tar.gz\u001b[0m\r\n",
      "spacy_stopwords.txt\r\n",
      "topic_word_distributions_10topics.csv\r\n",
      "topic_word_distributions_15topics.csv\r\n",
      "topic_word_distributions_20topics.csv\r\n",
      "topic_word_distributions_25topics.csv\r\n",
      "topic_word_distributions_50topics.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls times_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
